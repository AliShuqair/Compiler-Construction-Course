# Syntax Analysis(Parser)

A Syntax analyzer is formally defined as :

> An Algorithm that Groups the Set of Tokens Sent by the Scanner to Form
> **Syntax Structures** Such As Expressions, Statements, Blocks,etc.

Simply put, the parser examines if the source code written follows
the grammar(production rules) of the language.


The Syntax structure of programming languages and even spoken languages
can be expressed in what is called **BNF** notation, which stands 
for **B**akus **N**aur **F**orm. 

For example, in spoken English, we can say the following:

> sentence --> noun-phrase	verb-phrase
>
> noun-phrase --> article	noun 
>
> article --> THE | A | ...
> 
> noun --> STUDENT | BOOK | ...
>
> verb-phrase --> verb noun-phrase 
>
> verb --> READS | BUYS | ....

Note : The BNF Notation uses [different symbols](https://en.wikipedia.org/wiki/Backus%E2%80%93Naur_form#Example),
for example, a sentence is defined as :

> \< sentence \> ::= \< noun-phrase \>	\< verb-phrase \>

But this is very cumbersome, so we use the first notation, since its
easier to use. 

Now, let us derive a sentence : 

> sentence --> **noun-phrase** verb-phrase 
>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
> **article** noun verb-phrase
>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
> THE **noun** verb-phrase
>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
> THE STUDENT **verb-phrase**
>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
> THE STUDENT **verb** noun-phrase
>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
> THE STUDENT READS **noun-phrase**
> 
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
> THE STUDENT READS **article** noun
>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
> THE STUDENT READS A **noun**
>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
> THE STUDENT READS A BOOK


In the same way, the parser tries to **derive** your source program 
from the starting symbol of the grammar.

Lets say we have these sentences :

> THE BOOK BUYS A STUDENT
>
> THE BOOK WRITES A DISH
>
> THE DISH TAKES A STROLL

Syntax-wise, all of these sentences are correct. However, their meaning 
is not correct, and they are not useful. What differentiates 2
sentences that are grammatically correct is their meaning or their 
**semantics**. You and I can agree that the meaning of a grammatically 
correct sentence is not correct, but how does the computer do it?



## Grammar 

> A grammar G=(V<sub>N</sub>, V<sub>T</sub>, S, P) where:
>
> 1. V<sub>N</sub> : A finite set of nonterminals(nonterminals set).
> 2. V<sub>T</sub> : A finite set of terminals(terminals set).
> 3. S &isin; V<sub>N</sub> : The Starting symbol of the grammar. 
> 4. P =  A set of **production rules**(productions).<-- Pending <==> Basically the whole grammar.

Note :

1. V<sub>N</sub> &cap; V<sub>T</sub> = &empty;.
2. V<sub>N</sub> &cup; V<sub>T</sub> = V(the vocabulary of the grammar).

Note : We will use 

1. Uppercase Letters A,B,...,Z for non-terminals.

2. Lowercase Letters a,b,...,z for terminals.

3. Greek letters &alpha;,&beta;,&gamma;,... for strings formed from V<sub>N</sub> OR V<sub>T</sub> = V. eg, 

   if V<sub>N</sub> = {S,A,B},
   
   V<sub>T</sub> = {0,1}
   
   then
   
   &alpha; = 0A11B
   
   &beta; = S110B
   
   &gamma; = 0010

### Productions 

1. A Production &alpha; --> &beta;(alpha derives beta) is a rewriting rule such that
the occurrence of &alpha; can be substituted by &beta; in any string.

   Note that &alpha; must contain at least one nonterminal from,&isin;V<sub>N</sub>. 

   For example, Assume we have the string &gamma;&alpha;&sigma;,

   > &gamma;&alpha;&sigma; --> &gamma;&beta;&sigma;
   
2. A Derivation is a sequence of strings &alpha;<sub>0</sub>, &alpha;<sub>1</sub>,
&alpha;<sub>2</sub>, &alpha;<sub>3</sub>,....,&alpha;<sub>n</sub>, then :

	- &alpha;<sub>0</sub> -*-> &alpha;<sub>n</sub>, n &ge; 0.
	
	- &alpha;<sub>0</sub> -<sup>+</sup>-> &alpha;<sub>n</sub>, n &ge; 1.
	

Given a grammar G, then :

> L(G) = Language Generated By the Grammar.
	
for example, Given the Grammar, G = ({S,B,C},{a,b,c},S,P)

P :

> S --> aSBC 
>
> S --> abC
>
> CB --> BC
>
> bB --> bb
>
> bC --> bc
>
> cC --> CC

L(G)=?

Lets follow through on the derivations

> S --> a**bC** --> abc(all terminals) &isin; L(G) <--- A sentence
>
> S --> a**S**BC --> aa**bC**BC --> aabbcBC --> blocked, so we try another path
> 
> S --> a**S**BC --> aab**CB**C --> aa**bB**CC --> aab**bC**C --> aabb**cC** --> aabbcc &isin; L(G) <--- A sentence
>
> S --> a**S**BC -->........-->aaabbbccc &isin; L(G) <--- A sentence
>
> Therefore, L(G)={a<sup>n</sup>,b<sup>n</sup>,c<sup>n</sup>| n &ge; 1}


As another Example, we have these productions

> E --> E+T <-- we can write the productions 1 and 2 as a single production E --> E+T | T
>
> E --> T
>
> T --> T*F
>
> T --> F
>
> F --> (E) <-- we can write the productions 5 and 6 as a single production F --> (E) | n
>
> F --> n

Lets follow through some derivations

> E --> **T** --> **F** --> n &isin; L(E)
>
> E --> **E**+T --> T+**T** --> T+**F** --> **T**+n --> **F**+n --> n+n &isin; L(E)
>
> E --> **E**+T --> **T**+T --> **F**+T --> n+**T** --> n+**F** --> n + (**E**) --> n+(**T**)
> --> n+(**T**\*F) --> n+(**F**\*F) --> n+(n\***F**) --> n+(n\*n) &isin; L(E)
>
> Therefore, L(G) = {Any arithmetic expression with \* and + operations},
> n is an operand here.

Note that, if we add the productions

> E --> E+T | E-T | T
>
> T --> T\*F | T/F | T%F

We would have a language to express all arithmetic expressions with 
(\*,\\,+,\-) operations.

Lets Take another Example(things in double quotes are terminals)

> Program --> block "#"
>
> block --> "{" stmt-List "}"
>
> stmt-List --> statement ";" stmt-List | &lambda;
>
> statement --> if-stmt | while-stmt | read-stmt | write-stmt |
> assignment-stmt | block
>
> if-stmt --> "if" condition.... 
>
> while-stmt --> "while" condition.....
>
> ....
> 
> ....
> 
> read-stmt --> "read"
>
> write-stmt --> "write"


V<sub>N</sub> = {Program, block, stmt-List, statement, if-stmt,
while-stmt, read-stmt, write-stmt, assignment-stmt}

V<sub>T</sub> = { "{", "}", "#", ";", "if", "while", "read", "write" }


Lets Follow through some derivations :

> Program --> **block** # --> { **stmt-list** } # --> { &lambda; } #
>
> Program --> **block** # --> {**stmt-list**} # --> {statement ; **stmt-list**} #
> --> {statement ; statement ; **stmt-list**} # --> {statement ; statement ; &lambda;} #
> --> {**statement** ; statement ;} # -->  {**READ-statement** ; statement ;} #
> -->{READ ; **statement** ;} # -->{READ ; write-statement ;} # --> {READ ; WRITE ;} #

We can write this as 

```
{ READ;
  WRITE;
}#
```
The language of this language is defined as 

> L(G) = {Set of all programs that can be written in this language}.

This is only a simple example, of a simple language. For something more
complex such as C or Pascal, there are hundreds of productions.


### Algorithms for Derivation 


>A Leftmost derivation is a derivation in which we replace the **leftmost**
>nonterminal in each derivation step.


>A Rightmost derivation is a derivation in which we replace the **rightmost**
>nonterminal in each derivation step.

For example, given the grammar

>V --> S R $
>
>S --> +|-|&lambda;
>
>R --> .dN | dN.N
>
>N --> dN | &lambda;
>
>V<sub>N</sub> = {V,R,S,N}
>
>V<sub>T</sub> = {+, - , ., d, $}

Lets follow through on the leftmost derivation 

> V --> **S**R$ --> -**R**$ --> -d**N**.N$ --> -dd**N**.N$ --> -dddN.N$
> --> -ddd.**N**$ --> -ddd.d**N**$ --> -ddd.d$ <-- A sentence.

Lets follow through on the rightmost derivation

> V --> S**R**$ --> SdN.**N** --> SdN.d**N**$ --> Sd**N**.d$ --> sdd**N**.d$
> --> sddd**N**.d$ --> **S**ddd.d$ --> -ddd.d$ <-- A sentence.

### Derivation Trees

A Derivation Tree is a Tree that displays the derivation of some 
sentence in the language. For example, lets look at the 
tree for the previous example

**INSERT IMAGE OF TREE FOR -ddd.d$**

Note that if we traverse the tree in order, recording **only** the leaves,
we obtain the sentence.

### Classes of Grammars 

According to Chomsky, Grammars can be classified into :

1. Unrestricted Grammars : No restrictions whatsoever. It is not 
practical to work with.

2. 

## Parsing Techniques

There are 2 main parsing techniques used by a compiler.

### Top-Down Parsing

In Top-Down Parsing, the parser builds the derivation tree from the 
root(S : the starting symbol) down to the leaves(sentence). 

In Simple words, the parser tries to derive the sentence using 
leftmost derivation. For example, say we have this grammar :

> V --> SR$
>
> S --> + | - | &lambda;
>
> R --> .dN | dN.N
>
> N --> dN | &lambda;

Lets examine if the sentence

> dd.d$ 

is derived from this grammar.

> V --> **S**R$ --> +**R**$ --> d**N**.N$ --> dd**N**.N$ --> dd.**N**$ --> dd.d**N**$ --> dd.d$

Therefore, this sentence is derived from the grammar.

However, this approach is very computationally intensive, and more importantly, 
this requires knowing the source code in advance. The Parser doesnt know
which production it should select in each derivation statement. We will
learn how to solve these issues later in the course.

### Bottom-Up Parsing

In Bottom-Up Parsing, the parser builds the derivation tree from the
leaves(sentence) up to the root(S : Starting Symbol). This type of tree,
built from the leaves to the root, is 
called a [B-Tree](https://en.wikipedia.org/wiki/B-tree).

In Simple words, the parser starts with the given sentence, does 
**reduction**(opposite of derivation) steps, until the starting symbol
is reached.

Note that the string &lambda; is present everywhere in the string, and
we can use it wherever we like.

Lets follow the reduction of the example given above.

> +dd.d$ --> +dd&lambda;.d$ --> +ddN.d$ --> +dN.d$ --> +dN.d&lambda;$
> --> +dN.dN$ --> +dN.N$ --> +R$ --> SR$ --> V

Which means that the sentence is in the grammar. 

Note that we can run into deadlocks here. say we took this path instead :

> +dd.d$ --> +dd&lambda;.d$ --> +ddN.d$ --> +dN.d$ --> +dN.d&lambda;$
> --> +dN.dN$ --> **+dNR$ --> +NR$ --> SNR$** --> Deadlock

This technique also has a major problem : Which substring should we 
select to reduce in each reduction step?

how do we slove this?

## Ambiguity 

Given the following grammar :

> num --> num d
>
> num --> d

Let us draw the derivation tree for the sentence ```dddd```

** TODO INSERT TREE**

Question : is there another derivation tree that represents the sentence?

The answer is **no**.

If there is only one derivation tree representing the sentence, 
this means there is only one way to derive the sentence.

Based on this, we can say that :

> A Grammar G is said to be ambiguous if there is one sentence with more than
> one derivation tree. 
> 
> That is, there is more than one way to derive the sentence. 
>
> This means that our algorithm is **non-deterministic**.

Say we have this grammar

> E --> E + E
>
> E --> E * E
>
> E --> (E) | a

Take the sentence :

> a + a * a

Lets draw the derivation tree

**TODO INSERT DERIVATION TREE 1 and 2**


Due to the fact that we have 2 trees that give the same result, we can 
say that this grammar is ambiguous.

In this case, to enforce the associativity rule, this grammar can 
be re-written as :

> E --> E + E | T 
>
> T --> T*T | F
>
> F--> (E) | a

Now, Take the sentence ```a + a * a```
and find the derivation tree now. 

** INSERT NEW DERIVATION TREE **

There is only 1 possible derivation tree now. This solves the associativity
issue of the grammar before with the ```+``` and ```*``` operations.

But lets say we have the sentence :

> a + a + a

Lets try to find the derivation tree and any alternative trees. 

**TODO INSERT DERIVATION TREES**

We can see here that there is more than 1 derivation tree, and the 
language is still ambiguous.

We can solve this if we rewrite the grammar with the **left-associative rule**

> E --> E + T | T
>
> T --> T * F | F
>
> F --> (E) | a

The resultant grammar is left-associative.

This grammar solves the problems of :

- ambiguity.
- precedence.
- associativity.

Lets try rewriting it with the **right-associative rule**

> E --> T + E | T
>
> T --> F * T | F
>
> F --> (E) | a

Lets try creating the derivation tree of ```a + a * a```

** INSERT THE TREE of a+a*a**

Now lets draw the derivation tree of ```a + a + a```

** INSERT THE TREE OF a+a+a**

This new grammar is not ambiguious, however, as we can tell from the derivation 
trees, there are precedence issues now. It's not technically wrong, 
but it doesnt not follow standard arithmetic rules.

Back to the left-associative grammar now. This grammar is called
**left-recursive**. This causes problems when it omes to top-down parsing
techniques(we will see why later).

A grammar is said to be left recursive if there is a production of the form:

> A-->A&alpha;

Conversely, a grammar is right-recursive if there is a production of the form:

> A-->&alpha;A

And causes no problems in top-down parsing.

our grammar has 2 rules of the form 

>A-->A&alpha;

The solution is to transform the grammar to a grammar which is not 
left-recursive. 

This has an algorithm to it. 

Given that 

> A-->A&alpha;<sub>1</sub>| A&alpha;<sub>2</sub>|A&alpha;<sub>3</sub>|...|A&alpha;<sub>n</sub>
>
> A-->&Beta;<sub>1</sub>|&Beta;<sub>2</sub>|&Beta;<sub>3</sub>|...|&Beta;<sub>n</sub>

To do this, we must introduce a new non-terminal, say A\`.

The grammar now becomes :

> A-->&Beta;<sub>1</sub>A\`|&Beta;<sub>2</sub>A\`|&Beta;<sub>3</sub>A\`|...|&Beta;<sub>n</sub>A\`

and

> A\`-->&alpha;<sub>1</sub>A\`| &alpha;<sub>2</sub>A\`|&alpha;<sub>3</sub>A\`|...|&alpha;<sub>n</sub>A\`|&lambda;

For example, say we have

>A-->Ab
>
>A-->a
>
>L(G)=ab*

Then according to the above

>A-->aA\`
>
>A\`-->bA\`|&lambda;

which results in the same grammar.

Lets apply this to the grammar :

> E --> E + T | T
>
> T --> T * F | F
>
> F --> (E) | a

This results in :

> E --> T E\` 
>
> E\` --> + T E\` | &lambda;
>
> T --> F T\`
>
> T\` --> \* F T\` | &lambda;
>
> F --> (E) | a

This grammar is now **perfect**. It solves all our ambiguity issues, and this is a
grammar we can use to construct the production rules for our programming
language.

Another ambiguity in programming languages is the ```if...else```
statement.

Lets take a generic if statement in a generic language:

> stmt --> if-stmt | while-stmt | ....
>
> if-stmt --> IF condition stmt
>
> if-stmt --> IF condition stmt ELSE stmt 
>
> condition --> C
>
> stmt --> S 

This grammar is ambiguous.

Lets take the nested ```if...else``` statement :

```
IF C
	IF C
		S
	ELSE
		S
		
```
This statement results in 2 derivations trees :

**INSERT TREES OF THIS STATEMENT**

Both these trees result in the same traversal, but they have different meanings.
The first results in the ```ELSE``` belonging to the first ```IF```, while the 
second results int he ```ELSE``` belonging to the second ```IF```. We as 
humans know that the ```ELSE``` belongs to the second ```IF```, since we know that
the ```ELSE``` statement follows the nearest ```IF```. but how can 
the compiler know?

There are a bunch of solutions to this problem:

1. Add a delimiter to the ```IF``` statement, such as ```ENDIF``` or 
```END``` or ```FI``` to the end of the statement, resulting in these 
productions :
> if-stmt --> IF condition stmt **ENDIF**
>
> if-stmt --> IF condition stmt ELSE stmt **ENDIF**

	Resulting in this statement :
	
	```
	IF C
	|	IF C
	|	|	S
	|	|ELSE
	|	|	S
	|	ENDIF
	ENDIF
	
	```
	The grammar is now unambigious, since we have to clearly state when 
	an ```IF``` statement ends. However, this is not a pretty solution, 
	and is extra work for both the programmar and compiler, and results
	in less readable code.
2. In C and Pascal, the compiler **always** prefers to shift the ```ELSE```
	when it sees it in the source code so it follows the nearest ```IF```. We 
	will learn about this in more detail later.
	
Another thing about this grammar is **left factoring**. Consider the 
productions :

> A --> &alpha;&beta;
>
> A --> &alpha;&gamma;

Note how the first part of the productions is the same. This grammar
can be transformed by introducing a new non-terminal, So what happens now is:

> A --> &alpha;B
>  
> B --> &beta;&gamma;

For our grammar, this results in 

> if-stmt --> IF condition stmt
>
> if-stmt --> IF condition stmt ELSE stmt 

becoming

> if-stmt --> IF conditon stmt else-part
>
> else-parte --> ELSE stmt | &lambda;

Does this solve the ambiguity? No, but it helps.
