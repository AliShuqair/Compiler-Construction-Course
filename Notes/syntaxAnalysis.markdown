# Syntax Analysis(Parser)

A Syntax analyzer is formally defined as :

> An Algorithm that Groups the Set of Tokens Sent by the Scanner to Form
> **Syntax Structures** Such As Expressions, Statements, Blocks,etc.

Simply put, the parser examines if the source code written follows
the grammar(production rules) of the language.


The Syntax structure of programming languages and even spoken languages
can be expressed in what is called **BNF** notation, which stands 
for **B**akus **N**aur **F**orm. 

For example, in spoken English, we can say the following:

> sentence --> noun-phrase	verb-phrase
>
> noun-phrase --> article	noun 
>
> article --> THE | A | ...
> 
> noun --> STUDENT | BOOK | ...
>
> verb-phrase --> verb noun-phrase 
>
> verb --> READS | BUYS | ....

Note : The BNF Notation uses [different symbols](https://en.wikipedia.org/wiki/Backus%E2%80%93Naur_form#Example),
for example, a sentence is defined as :

> \< sentence \> ::= \< noun-phrase \>	\< verb-phrase \>

But this is very cumbersome, so we use the first notation, since its
easier to use. 

Now, let us derive a sentence : 

> sentence --> **noun-phrase** verb-phrase 
>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
> **article** noun verb-phrase
>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
> THE **noun** verb-phrase
>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
> THE STUDENT **verb-phrase**
>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
> THE STUDENT **verb** noun-phrase
>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
> THE STUDENT READS **noun-phrase**
> 
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
> THE STUDENT READS **article** noun
>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
> THE STUDENT READS A **noun**
>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
> THE STUDENT READS A BOOK


In the same way, the parser tries to **derive** your source program 
from the starting symbol of the grammar.

Lets say we have these sentences :

> THE BOOK BUYS A STUDENT
>
> THE BOOK WRITES A DISH
>
> THE DISH TAKES A STROLL

Syntax-wise, all of these sentences are correct. However, their meaning 
is not correct, and they are not useful. What differentiates 2
sentences that are grammatically correct is their meaning or their 
**semantics**. You and I can agree that the meaning of a grammatically 
correct sentence is not correct, but how does the computer do it?



## Grammar 

> A grammar G=(V<sub>N</sub>, V<sub>T</sub>, S, P) where:
>
> 1. V<sub>N</sub> : A finite set of nonterminals(nonterminals set).
> 2. V<sub>T</sub> : A finite set of terminals(terminals set).
> 3. S &isin; V<sub>N</sub> : The Starting symbol of the grammar. 
> 4. P =  A set of **production rules**(productions).<-- Pending <==> Basically the whole grammar.

Note :

1. V<sub>N</sub> &cap; V<sub>T</sub> = &empty;.
2. V<sub>N</sub> &cup; V<sub>T</sub> = V(the vocabulary of the grammar).

Note : We will use 

1. Uppercase Letters A,B,...,Z for non-terminals.

2. Lowercase Letters a,b,...,z for terminals.

3. Greek letters &alpha;,&beta;,&gamma;,... for strings formed from V<sub>N</sub> OR V<sub>T</sub> = V. eg, 

   if V<sub>N</sub> = {S,A,B},
   
   V<sub>T</sub> = {0,1}
   
   then
   
   &alpha; = 0A11B
   
   &beta; = S110B
   
   &gamma; = 0010

### Productions 

1. A Production &alpha; --> &beta;(alpha derives beta) is a rewriting rule such that
the occurrence of &alpha; can be substituted by &beta; in any string.

   Note that &alpha; must contain at least one nonterminal from,&isin;V<sub>N</sub>. 

   For example, Assume we have the string &gamma;&alpha;&sigma;,

   > &gamma;&alpha;&sigma; --> &gamma;&beta;&sigma;
   
2. A Derivation is a sequence of strings &alpha;<sub>0</sub>, &alpha;<sub>1</sub>,
&alpha;<sub>2</sub>, &alpha;<sub>3</sub>,....,&alpha;<sub>n</sub>, then :

	- &alpha;<sub>0</sub> -*-> &alpha;<sub>n</sub>, n &ge; 0.
	
	- &alpha;<sub>0</sub> -<sup>+</sup>-> &alpha;<sub>n</sub>, n &ge; 1.
	

Given a grammar G, then :

> L(G) = Language Generated By the Grammar.
	
for example, Given the Grammar, G = ({S,B,C},{a,b,c},S,P)

P :

> S --> aSBC 
>
> S --> abC
>
> CB --> BC
>
> bB --> bb
>
> bC --> bc
>
> cC --> CC

L(G)=?

Lets follow through on the derivations

> S --> a**bC** --> abc(all terminals) &isin; L(G) <--- A sentence
>
> S --> a**S**BC --> aa**bC**BC --> aabbcBC --> blocked, so we try another path
> 
> S --> a**S**BC --> aab**CB**C --> aa**bB**CC --> aab**bC**C --> aabb**cC** --> aabbcc &isin; L(G) <--- A sentence
>
> S --> a**S**BC -->........-->aaabbbccc &isin; L(G) <--- A sentence
>
> Therefore, L(G)={a<sup>n</sup>,b<sup>n</sup>,c<sup>n</sup>| n &ge; 1}


As another Example, we have these productions

> E --> E+T <-- we can write the productions 1 and 2 as a single production E --> E+T | T
>
> E --> T
>
> T --> T*F
>
> T --> F
>
> F --> (E) <-- we can write the productions 5 and 6 as a single production F --> (E) | n
>
> F --> n

Lets follow through some derivations

> E --> **T** --> **F** --> n &isin; L(E)
>
> E --> **E**+T --> T+**T** --> T+**F** --> **T**+n --> **F**+n --> n+n &isin; L(E)
>
> E --> **E**+T --> **T**+T --> **F**+T --> n+**T** --> n+**F** --> n + (**E**) --> n+(**T**)
> --> n+(**T**\*F) --> n+(**F**\*F) --> n+(n\***F**) --> n+(n\*n) &isin; L(E)
>
> Therefore, L(G) = {Any arithmetic expression with \* and + operations},
> n is an operand here.

Note that, if we add the productions

> E --> E+T | E-T | T
>
> T --> T\*F | T/F | T%F

We would have a language to express all arithmetic expressions with 
(\*,\\,+,\-) operations.

Lets Take another Example(things in double quotes are terminals)

> Program --> block "#"
>
> block --> "{" stmt-List "}"
>
> stmt-List --> statement ";" stmt-List | &lambda;
>
> statement --> if-stmt | while-stmt | read-stmt | write-stmt |
> assignment-stmt | block
>
> if-stmt --> "if" condition.... 
>
> while-stmt --> "while" condition.....
>
> ....
> 
> ....
> 
> read-stmt --> "read"
>
> write-stmt --> "write"


V<sub>N</sub> = {Program, block, stmt-List, statement, if-stmt,
while-stmt, read-stmt, write-stmt, assignment-stmt}

V<sub>T</sub> = { "{", "}", "#", ";", "if", "while", "read", "write" }


Lets Follow through some derivations :

> Program --> **block** # --> { **stmt-list** } # --> { &lambda; } #
>
> Program --> **block** # --> {**stmt-list**} # --> {statement ; **stmt-list**} #
> --> {statement ; statement ; **stmt-list**} # --> {statement ; statement ; &lambda;} #
> --> {**statement** ; statement ;} # -->  {**READ-statement** ; statement ;} #
> -->{READ ; **statement** ;} # -->{READ ; write-statement ;} # --> {READ ; WRITE ;} #

We can write this as 

```
{ READ;
  WRITE;
}#
```
The language of this language is defined as 

> L(G) = {Set of all programs that can be written in this language}.

This is only a simple example, of a simple language. For something more
complex such as C or Pascal, there are hundreds of productions.


### Algorithms for Derivation 


>A Leftmost derivation is a derivation in which we replace the **leftmost**
>nonterminal in each derivation step.


>A Rightmost derivation is a derivation in which we replace the **rightmost**
>nonterminal in each derivation step.

For example, given the grammar

>V --> S R $
>
>S --> +|-|&lambda;
>
>R --> .dN | dN.N
>
>N --> dN | &lambda;
>
>V<sub>N</sub> = {V,R,S,N}
>
>V<sub>T</sub> = {+, - , ., d, $}

Lets follow through on the leftmost derivation 

> V --> **S**R$ --> -**R**$ --> -d**N**.N$ --> -dd**N**.N$ --> -dddN.N$
> --> -ddd.**N**$ --> -ddd.d**N**$ --> -ddd.d$ <-- A sentence.

Lets follow through on the rightmost derivation

> V --> S**R**$ --> SdN.**N** --> SdN.d**N**$ --> Sd**N**.d$ --> sdd**N**.d$
> --> sddd**N**.d$ --> **S**ddd.d$ --> -ddd.d$ <-- A sentence.

### Derivation Trees

A Derivation Tree is a Tree that displays the derivation of some 
sentence in the language. For example, lets look at the 
tree for the previous example

**INSERT IMAGE OF TREE FOR -ddd.d$**

Note that if we traverse the tree in order, recording **only** the leaves,
we obtain the sentence.

### Classes of Grammars 

According to Chomsky, Grammars can be classified into :

1. Unrestricted Grammars : No restrictions whatsoever. It is not 
practical to work with.

2. 

## Parsing Techniques

There are 2 main parsing techniques used by a compiler.

### Top-Down Parsing

In Top-Down Parsing, the parser builds the derivation tree from the 
root(S : the starting symbol) down to the leaves(sentence). 

In Simple words, the parser tries to derive the sentence using 
leftmost derivation. For example, say we have this grammar :

> V --> SR$
>
> S --> + | - | &lambda;
>
> R --> .dN | dN.N
>
> N --> dN | &lambda;

Lets examine if the sentence

> dd.d$ 

is derived from this grammar.

> V --> **S**R$ --> +**R**$ --> d**N**.N$ --> dd**N**.N$ --> dd.**N**$ --> dd.d**N**$ --> dd.d$

Therefore, this sentence is derived from the grammar.

However, this approach is very computationally intensive, and more importantly, 
this requires knowing the source code in advance. The Parser doesnt know
which production it should select in each derivation statement. We will
learn how to solve these issues later in the course.

### Bottom-Up Parsing

In Bottom-Up Parsing, the parser builds the derivation tree from the
leaves(sentence) up to the root(S : Starting Symbol). This type of tree,
built from the leaves to the root, is 
called a [B-Tree](https://en.wikipedia.org/wiki/B-tree).

In Simple words, the parser starts with the given sentence, does 
**reduction**(opposite of derivation) steps, until the starting symbol
is reached.

Note that the string &lambda; is present everywhere in the string, and
we can use it wherever we like.

Lets follow the reduction of the example given above.

> +dd.d$ --> +dd&lambda;.d$ --> +ddN.d$ --> +dN.d$ --> +dN.d&lambda;$
> --> +dN.dN$ --> +dN.N$ --> +R$ --> SR$ --> V

Which means that the sentence is in the grammar. 

Note that we can run into deadlocks here. say we took this path instead :

> +dd.d$ --> +dd&lambda;.d$ --> +ddN.d$ --> +dN.d$ --> +dN.d&lambda;$
> --> +dN.dN$ --> **+dNR$ --> +NR$ --> SNR$** --> Deadlock

This technique also has a major problem : Which substring should we 
select to reduce in each reduction step?

how do we slove this?

## Ambiguity 

Given the following grammar :

> num --> num d
>
> num --> d

Let us draw the derivation tree for the sentence ```dddd```

** TODO INSERT TREE**

Question : is there another derivation tree that represents the sentence?

The answer is **no**.

If there is only one derivation tree representing the sentence, 
this means there is only one way to derive the sentence.

Based on this, we can say that :

> A Grammar G is said to be ambiguous if there is one sentence with more than
> one derivation tree. 
> 
> That is, there is more than one way to derive the sentence. 
>
> This means that our algorithm is **non-deterministic**.

Say we have this grammar

> E --> E + E
>
> E --> E * E
>
> E --> (E) | a

Take the sentence :

> a + a * a

Lets draw the derivation tree

**TODO INSERT DERIVATION TREE 1 and 2**


Due to the fact that we have 2 trees that give the same result, we can 
say that this grammar is ambiguous.

In this case, to enforce the associativity rule, this grammar can 
be re-written as :

> E --> E + E | T 
>
> T --> T*T | F
>
> F--> (E) | a

Now, Take the sentence ```a + a * a```
and find the derivation tree now. 

** INSERT NEW DERIVATION TREE **

There is only 1 possible derivation tree now. This solves the associativity
issue of the grammar before with the ```+``` and ```*``` operations.

But lets say we have the sentence :

> a + a + a

Lets try to find the derivation tree and any alternative trees. 

**TODO INSERT DERIVATION TREES**

We can see here that there is more than 1 derivation tree, and the 
language is still ambiguous.

We can solve this if we rewrite the grammar with the **left-associative rule**

> E --> E + T | T
>
> T --> T * F | F
>
> F --> (E) | a

The resultant grammar is left-associative.

This grammar solves the problems of :

- ambiguity.
- precedence.
- associativity.

Lets try rewriting it with the **right-associative rule**

> E --> T + E | T
>
> T --> F * T | F
>
> F --> (E) | a

Lets try creating the derivation tree of ```a + a * a```

** INSERT THE TREE of a+a*a**

Now lets draw the derivation tree of ```a + a + a```

** INSERT THE TREE OF a+a+a**

This new grammar is not ambiguity, however, as we can tell from the derivation 
trees, there are precedence issues now. It's not technically wrong, 
but it doesnt not follow standard arithmetic rules.
